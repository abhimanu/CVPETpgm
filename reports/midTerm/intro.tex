\section{Introduction}

Graphical models have been extensively used in recent decades in the broader
research area of machine learning: from linguistics to network discovery and
structure prediction in parsers to online social
media~\cite{Koller+Friedman:09} Graphical models provide a convenient way of
representing complex structures.  They are a great tool to represent data
which has an intuitive generative or a causal story~\cite{GettingStarted}. In
most of these cases we are estimating the parameters of the probabilistic
model where some of the member nodes of the graphs are not observed or hidden.
Models like these are also very useful in the cases when one has a causal
story where there are hidden states of the in the causal chain for e.g. hidden
markov model~\cite{Baum1967}. Graphical modelling is one of the most effective
way to estimate models that involve transitions over a state space e.g.
temporal modelling \cite{Arnold:2007:TCM:1281192.1281203}. 

There have been various methods proposed for estimating parameters of such a
model e.g. sampling techniques such as Gibbs, Metropolis-Hasting
etc.~\cite{Robert:2005:MCS:1051451}, variational inference
techniques~\cite{citeulike:6420690}, Laplace
approximation~\cite{Azevedo-Filho:1994} etc. Each estimation procedure comes
with its own advantages and disadvantages. For instance, it is generally
believed that sampling stratagies are more accurate than variational or
Laplace, but the better estimation comes with a price of slower computation.
It is also observed that many sampling strategies suffer from poor mixing of
chains for high dimensional data~\cite{ShenACOSB10}. Laplace approximation
tries to approximate potentially complex posterior distribution with
multivariate Gaussian. Although the results are poorer than variational and
sampling, Laplace approximation is often computationally faster in most cases.
Despite evidences of these trade-offs~\cite{Asuncion2009smoothing,
medlda_MCMC12}, to the best of our knowledge, there is no comprehensive
empirical studies of these approximation techniques on classification tasks. 

Graphical models also have variations in terms of the objective one wants to
optimize. We can optimize the likelihood (MLE) of the model directly or a
sparse approximation of the likelihood~\cite{Banerjee:2008}. We can also put a
prior on a set of parameters and obtain a model that maximizes the posterior
of the variables using various optimization techniques~\cite{abs-0710-0013}.
We use priors for the parameters mostly due to two reasons: 1) we have a prior
belief about the parameters, or 2) we want a sparse estimation of the
model~\cite{Yoshida:2010}. There are various quirks and tricks with lots of
unsaid rules that go in all these optimization techniques. Besides one can
estimate a graphical model by choosing any of these two objectives and
maximize it using any of the estimation techniques mentioned above. So we have
a whole gamut of possibilities about which not much is known. 

Our goal in this project is to clear the uncertainty about choice of the
objective and the corressponding estimation technique to choose. We aim to
perform an empirical study of various combinations of these startegies and
objectives to be optimized on a suitable graphical model problem. Our choice
of the problem is bayesian logistic regression~\cite{Xu:2008:BLR}. Bayesian
logistic regression is a widely used model and thus a suitable choice for this
particular analysis. We also plan to do comparitive study on other models such
as LDA etc. if time permits.

