\documentclass{article}

\usepackage{nips12submit_e, times}
\usepackage{hyperref}

\title{An Empirical Study of Approximate Inference Algorithms on Bayesian
Logistic Regression}

\author{
Wei Dai \\
School of Computer Science \\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{wdai@cs.cmu.edu}\\
\And
Abhimanu Kumar \\
School of Computer Science \\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{abhimank@cs.cmu.edu}\\
\And
Jinliang Wei \\
School of Computer Science \\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{jinlianw@cs.cmu.edu}\\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\nipsfinalcopy
\begin{document}
\maketitle

\setcounter{page}{1}
\pagenumbering{arabic}
% \vspace{-20pt}
% \section{Project Idea}
% \vspace{-5pt}

\input{intro}
\input{related_work}
\input{method}
\input{experiments}
\input{conclusion}


% Latent variable models have gained significant popularity due to their ability to
%  model data that potentially arise due to latent causes. These models are also
%   helpful in case of missing data as these missing entries can be modelled via 
%   hidden variables in graphical models. The parameter estimation for this class 
%   of models are in general intractable, and numerous approximation algorithms 
%   are widely employed. These approaches fall broadly into two categories: 1) 
%   variational inference, and 2) sampling-based. While it is generally believed 
%   that sampling produces better approximation than variational inference, albeit 
%   at a higher computational cost, to the best of our knowledge there is no 
%   comprehensive empirical study that compares these approximation inference schemes.
% 
% We plan to carry out an empirical comparative study of these approximation 
% algorithms on Bayesian logistic regression, a well-studied minimal model with 
% only one latent variable: the regression coefficients. The approximation 
% algorithms we will use are:
% \vspace{-5pt}
% \begin{enumerate}
%   \item Variational inference \cite{RePEc:bes:jnlasa:v:105:i:489:y:2010:p:324-335}:
%     \begin{enumerate}
%       \item A (near) close-form variational bound \cite{Jaakkola96avariational}
%       \item Laplace variational inference and delta method variational 
%       inference \cite{2012arXiv1209.4360W}
%     \end{enumerate}
%     \item Sampling-based: MCMC using Gibbs sampling
% \end{enumerate}
% \vspace{-5pt}
% We would apply the above approximation algorithms to the following estimation 
% problems for a comparative study:
% \vspace{-5pt}
% \begin{enumerate}
%   \item Maximum a posteriori (MAP) from variational and sampling algorithms
%   \item Maximum likelihood estimation (MLE)
%   \item Laplace approximation
% \end{enumerate}
% \vspace{-5pt}
% \section {Software, Datasets and Midterm Milstone}
% \vspace{-5pt}
% We plan to write all the code in matlab or C++ so that we have language agnostic 
% comparison results. We will use 5 
% \href{http://archive.ics.uci.edu/ml/datasets.html}{UCI classification datasets}: 
% 1) Farms Ads dataset (4,143 instances, 54,877 text features), 2) Amazon Commerce 
% reviews dataset (1,500 instances, 10,000 real features) 2) p53 Mutants dataset 
% (16,772 instances, 5,409 real attributes) 4) Human Activity Recognition using 
% Smartphones dataset (10,299 instances, 561 real features) 5) URL Reputation 
% dataset (2,396,130 instances, 3,231,961 real features).
% 
% We plan to finish MCMC sampling for all the three estimation by midterm.
% 
% See reference for papers to read.

\bibliographystyle{plain}
\bibliography{reference}

\end{document}
