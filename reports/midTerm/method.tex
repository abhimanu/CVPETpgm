Given data set $\{\phib_n, t_n\}_{n=1}^N$ where $\phib_n$ are the feature
vectors and $t_n\in \{0,1\}$ are the labels, we can write the likelihood
function for logistic regression as $p(\bm{t}|\bm{w}) = \prod_{n=1}^N
y_n^{t_n} (1-y_n)^{1-t_n}$ where $\bm{t} = (t_1,...,t_N)^T$ and
$y_n=p(\mathcal{C}_1|\phib_n) = \sigma(\bm{w}^T \phib_n)$ and $\sigma(s) =
\frac{1}{1+e^{-s}}$. Using Bayes rule, the posterior distribution over
$\bm{w}$ is $p(\bm{w}|\bm{t}) = \frac{p(\bm{w}) p(\bm{t}|\bm{w})}{p(\bm{t})}$
where $p(\bm{t}) = \int p(\bm{w})p(\bm{t}|\bm{w}) d\bm{w}$ involves logistic
sigmoid functions and is intractable. In the sequel we briefly describe three
approximation schemes (Laplace approximation, variational methods, and Gibbs
sampling), and point estimations (MLE, MAP) together with the associated
prediction rules.

\subsection{Laplace Approximation and associated MAP}

Laplace approximation approximate the posterior $p(\bm{w}|\bm{t})$ with a
multivariate Guassian $\mathcal{N}(\bm{w}; \bm{w}_{MAP}, \bm{S}_N)$ where
$\bm{w}_{MAP}$ is the maximum {\it a posteriori} and thus a mode of the
posterior and $\bm{S}_N^{-1} = -\nabla^2_{\bm{w}} \ln
p(\bm{w}|\bm{t})|_{\bm{w} = \bm{w}_{MAP}}$ is the Hessian at $\bm{w}_{MAP}$.
Since we are approximating the posterior with Gaussian, it is convenient to
use conjugate prior $p(\bm{w}) = \mathcal{N}(\bm{w};\bm{m}_0,\bm{S}_0)$. Thus
we have $\ln p(\bm{w}|\bm{t}) = -\frac{1}{2}(\bm{w}-\bm{m}_0)^T
\bm{S}_0^{-1}(\bm{w}-\bm{m}_0) + \sum_{n=1}^N\{t_n \ln y_n +(1-y_n) \ln
(1-y_n)\} + const$

Under the Laplace approximation and the conjugated Gaussian prior, $\bm{w}_{MAP}$
can be efficiently obtained by gradient descent. To encourage small
$||w||^2_2$, let $\bm{m}_0 = \bm{0}$, and $\bm{S}_0 = \sigma^2 \bm{I}$, we
have the following gradient descent rule: 

\begin{equation}
\bm{w}_t \leftarrow \bm{w}_{t-1} + \eta\left( \sum_{n=1}^N (t_n - y_{n,(t-1)}) \phib_n -
\frac{1}{\sigma^2}\bm{w}_{t-1} \right)
\end{equation}

where $\eta$ is the learning rate constant. We can also get

\begin{equation}
\bm{S}_N^{-1} = -\nabla^2_{\bm{w}} \ln p(\bm{w}|\bm{t})
= \bm{S}_0^{-1} + \sum_{n=1}^N y_n(1-y_n) \phib_n \phib_n^T
\end{equation}

The predictive distribution for Laplace-approximated posterior is not close
form, but by approximating the logistic sigmoid function with probit function,
we recover $\bm{w}_{MAP}$ as the decision boundary.

