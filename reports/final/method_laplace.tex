
\subsection{Laplace Approximation and associated MAP}

Laplace approximation approximate the posterior $p(\bm{w}|\bm{t})$ with a
multivariate Guassian $\mathcal{N}(\bm{w}; \bm{w}_{MAP},\Sigma_N)$ where
$\wb_{MAP}$ is the maximum {\it a posteriori} and thus a mode of the
posterior and $\Sigma_N^{-1} = -\nabla^2_{\bm{w}} \ln
p(\bm{w}|\bm{t})|_{\bm{w} = \bm{w}_{MAP}}$ is the Hessian at $\bm{w}_{MAP}$.
Since we are approximating the posterior with Gaussian, it is convenient to
use conjugate prior $p(\bm{w}) = \mathcal{N}(\bm{w};\bm{m}_0,\Sigma_0)$. Thus
we have
\begin{equation}
\ln p(\bm{w}|\bm{t}) = -\frac{1}{2}(\bm{w}-\bm{m}_0)^T
\Sigma_0^{-1}(\bm{w}-\bm{m}_0) + \sum_{n=1}^N\{t_n \ln y_n +(1-y_n) \ln
(1-y_n)\} + const
\end{equation}

Under the Laplace approximation and the conjugated Gaussian prior, $\bm{w}_{MAP}$
can be efficiently obtained by gradient descent. To encourage small
$||w||^2_2$, let $\bm{m}_0 = \bm{0}$, and $\Sigma_0 = \sigma^2 \bm{I}$, we
have the following gradient descent rule: 

\begin{equation}
\bm{w}_t \leftarrow \bm{w}_{t-1} + \eta\left( \sum_{n=1}^N (t_n - y_{n,(t-1)}) \phib_n -
\frac{1}{\sigma^2}\bm{w}_{t-1} \right)
\end{equation}

where $\eta$ is the learning rate constant. We can also get

\begin{equation}
\Sigma_N^{-1} = -\nabla^2_{\bm{w}} \ln p(\bm{w}|\bm{t})
= \Sigma_0^{-1} + \sum_{n=1}^N y_n(1-y_n) \phib_n \phib_n^T
\end{equation}

The predictive distribution for Laplace-approximated posterior is not close
form, but by approximating the logistic sigmoid function with probit function,
we recover $\bm{w}_{MAP}$ as the decision boundary.
