
\section{Conclusion an Future Work}
As we can see Kolmogorov Smirnov based sampling performs
considerable well compared to all other methods in terms of raw accuracy of
prediction. Though it is slower in speed compared to other variational methods
such as Laplace and Jordan's method. Sampling is harder to tune due to
asymptotic nature of the approach. It also suffers from bad-mixing if the
feature space is very large as well as when there is high correlaton among
successive particles sampled. This slow mixing can be clearly observed in the
uniform distribution based sampling approach for Bayesian logistic regression.
Sampling requires not only high degree of parameter tuning it also needs good
proposal distributions which are harder to find especially in case of
non-conjugacy. 

On the other hand we see that variational based approximations are considerable
fast (except delta method) but they perform poorly. This is understandable since
they maximize a lower bound on the original objective (log-likelihood) compared
to sampling which maximizes the log-likelihood itself. the variational updates
coming from an optimization family of approcahes suffer from the typical
step-size selection problem. Tuning this step size seems specially important in
this case since there are many more parameters in case of graphical models than
anyother typical optimization objective. The slow speed as well poor
converegence of delta method is a peculiar observation. We believe that delta is
highly prone to tuning of its various parameters as well as initial points and
thus behaves poorly. Though Laplace approaximation is a special case of delta it
performs considerable better than the delta. 

The three algorithms used in this paper almost surely cover the
span of various styles of approximations used in the graphical model
estimations. There are other approaches besides these such as
loopy belief propagation for general graphs but these are variants of the 
variational methods covered here~\cite{Heskes02}. We used a non-trivial
estimation problem involving a non-conjguate distribution set and by clever
approximation either via gaussians or Kolmogorov Smirnov obtained reliable and
accurate approximation techniques based on the approaches present in the
literature.

As we know sampling based techniques perform not so well in high dimensions. A
future extension of the work would be to evaluate sampling techniques on high
dimensional models. This can be achieved either by increasing the feature space
in the current Bayesian logistic regression problem or a different high
dimensional model altogether such as conditional random
fields~\cite{sutton06introduction} or some
other model with complex structured feature space with high dimensions. Since
variational based methods generaly perform well in higher dimension there is a
a likelihood that they might beat sampling not just in speed but accuracy too. 
Another interesting dimension to explore would be a comparison between
distributed version of these approximation techniques. Since the biggest
drawback of sampling based methods is their slow speed of converegence, a
distributed sampling strategy might be faster as well accurate and thus a better
alternative to other techniques. 

%     But sampling has many parameters to be tuned
% 
%     Variational schemes are sensitive to step size of the ascent while optimizing
% 
%     MCMC based sampling is slower than most variational inference (except delta method).
%     But can be fast when there is good mixing in the chains.